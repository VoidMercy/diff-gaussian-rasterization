//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32688072
// Cuda compilation tools, release 12.1, V12.1.105
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_52
.address_size 64

	// .globl	_ZNSt16initializer_listIjEC1EPKjm
.const .align 8 .b8 params[64];
.global .align 1 .b8 _ZN38_INTERNAL_5fa5120a_8_optix_cu_debcdc516thrust6system6detail10sequential3seqE[1];
.global .align 1 .b8 _ZN38_INTERNAL_5fa5120a_8_optix_cu_debcdc516thrust6system3cpp3parE[1];
.global .align 1 .b8 _ZN38_INTERNAL_5fa5120a_8_optix_cu_debcdc516thrust8cuda_cub3parE[1];
.global .align 1 .b8 _ZN38_INTERNAL_5fa5120a_8_optix_cu_debcdc516thrust8cuda_cub10par_nosyncE[1];
.global .align 1 .b8 _ZN38_INTERNAL_5fa5120a_8_optix_cu_debcdc516thrust12placeholders2_1E[1];
.global .align 1 .b8 _ZN38_INTERNAL_5fa5120a_8_optix_cu_debcdc516thrust12placeholders2_2E[1];
.global .align 1 .b8 _ZN38_INTERNAL_5fa5120a_8_optix_cu_debcdc516thrust12placeholders2_3E[1];
.global .align 1 .b8 _ZN38_INTERNAL_5fa5120a_8_optix_cu_debcdc516thrust12placeholders2_4E[1];
.global .align 1 .b8 _ZN38_INTERNAL_5fa5120a_8_optix_cu_debcdc516thrust12placeholders2_5E[1];
.global .align 1 .b8 _ZN38_INTERNAL_5fa5120a_8_optix_cu_debcdc516thrust12placeholders2_6E[1];
.global .align 1 .b8 _ZN38_INTERNAL_5fa5120a_8_optix_cu_debcdc516thrust12placeholders2_7E[1];
.global .align 1 .b8 _ZN38_INTERNAL_5fa5120a_8_optix_cu_debcdc516thrust12placeholders2_8E[1];
.global .align 1 .b8 _ZN38_INTERNAL_5fa5120a_8_optix_cu_debcdc516thrust12placeholders2_9E[1];
.global .align 1 .b8 _ZN38_INTERNAL_5fa5120a_8_optix_cu_debcdc516thrust12placeholders3_10E[1];
.global .align 1 .b8 _ZN38_INTERNAL_5fa5120a_8_optix_cu_debcdc516thrust3seqE[1];
.global .align 1 .b8 _ZN38_INTERNAL_5fa5120a_8_optix_cu_debcdc516thrust6deviceE[1];

.visible .func _ZNSt16initializer_listIjEC1EPKjm(
	.param .b64 _ZNSt16initializer_listIjEC1EPKjm_param_0,
	.param .b64 _ZNSt16initializer_listIjEC1EPKjm_param_1,
	.param .b64 _ZNSt16initializer_listIjEC1EPKjm_param_2
)
{
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_ZNSt16initializer_listIjEC1EPKjm_param_0];
	ld.param.u64 	%rd2, [_ZNSt16initializer_listIjEC1EPKjm_param_1];
	ld.param.u64 	%rd3, [_ZNSt16initializer_listIjEC1EPKjm_param_2];
	st.u64 	[%rd1], %rd2;
	st.u64 	[%rd1+8], %rd3;
	ret;

}
	// .globl	__raygen__rg
.visible .entry __raygen__rg()
{
	.local .align 16 .b8 	__local_depot1[4128];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .f32 	%f<70>;
	.reg .b32 	%r<161>;
	.reg .b64 	%rd<39>;


	mov.u64 	%SPL, __local_depot1;
	cvta.local.u64 	%SP, %SPL;
	add.u64 	%rd9, %SP, 0;
	add.u64 	%rd1, %SPL, 0;
	add.u64 	%rd10, %SP, 16;
	add.u64 	%rd2, %SPL, 16;
	add.u64 	%rd11, %SP, 4112;
	add.u64 	%rd3, %SPL, 4112;
	// begin inline asm
	call (%r44), _optix_get_launch_index_x, ();
	// end inline asm
	// begin inline asm
	call (%r45), _optix_get_launch_index_y, ();
	// end inline asm
	ld.const.v2.f32 	{%f10, %f11}, [params+24];
	mov.u32 	%r50, 0;
	add.ftz.f32 	%f14, %f10, %f10;
	cvt.rn.f32.s32 	%f15, %r44;
	add.ftz.f32 	%f16, %f15, 0f3F000000;
	ld.const.v2.u32 	{%r51, %r52}, [params+16];
	cvt.rn.f32.u32 	%f17, %r51;
	div.approx.ftz.f32 	%f18, %f16, %f17;
	mul.ftz.f32 	%f19, %f14, %f18;
	sub.ftz.f32 	%f20, %f19, %f10;
	add.ftz.f32 	%f21, %f11, %f11;
	cvt.rn.f32.s32 	%f22, %r45;
	add.ftz.f32 	%f23, %f22, 0f3F000000;
	cvt.rn.f32.u32 	%f24, %r52;
	div.approx.ftz.f32 	%f25, %f23, %f24;
	mul.ftz.f32 	%f26, %f21, %f25;
	sub.ftz.f32 	%f27, %f26, %f11;
	ld.const.u64 	%rd12, [params+32];
	cvta.to.global.u64 	%rd13, %rd12;
	ld.global.f32 	%f28, [%rd13];
	ld.global.f32 	%f29, [%rd13+16];
	mul.ftz.f32 	%f30, %f27, %f29;
	fma.rn.ftz.f32 	%f31, %f20, %f28, %f30;
	ld.global.f32 	%f32, [%rd13+32];
	add.ftz.f32 	%f33, %f31, %f32;
	ld.global.f32 	%f34, [%rd13+48];
	add.ftz.f32 	%f35, %f33, %f34;
	ld.global.f32 	%f36, [%rd13+4];
	ld.global.f32 	%f37, [%rd13+20];
	mul.ftz.f32 	%f38, %f27, %f37;
	fma.rn.ftz.f32 	%f39, %f20, %f36, %f38;
	ld.global.f32 	%f40, [%rd13+36];
	add.ftz.f32 	%f41, %f40, %f39;
	ld.global.f32 	%f42, [%rd13+52];
	add.ftz.f32 	%f43, %f41, %f42;
	ld.global.f32 	%f44, [%rd13+8];
	ld.global.f32 	%f45, [%rd13+24];
	mul.ftz.f32 	%f46, %f27, %f45;
	fma.rn.ftz.f32 	%f47, %f20, %f44, %f46;
	ld.global.f32 	%f48, [%rd13+40];
	add.ftz.f32 	%f49, %f48, %f47;
	ld.global.f32 	%f50, [%rd13+56];
	add.ftz.f32 	%f51, %f49, %f50;
	ld.const.u64 	%rd14, [params+40];
	cvta.to.global.u64 	%rd15, %rd14;
	ld.global.f32 	%f1, [%rd15];
	sub.ftz.f32 	%f52, %f35, %f1;
	ld.global.f32 	%f2, [%rd15+4];
	sub.ftz.f32 	%f53, %f43, %f2;
	ld.global.f32 	%f3, [%rd15+8];
	sub.ftz.f32 	%f54, %f51, %f3;
	mul.ftz.f32 	%f55, %f53, %f53;
	fma.rn.ftz.f32 	%f56, %f52, %f52, %f55;
	fma.rn.ftz.f32 	%f57, %f54, %f54, %f56;
	sqrt.approx.ftz.f32 	%f58, %f57;
	div.approx.ftz.f32 	%f4, %f52, %f58;
	div.approx.ftz.f32 	%f5, %f53, %f58;
	div.approx.ftz.f32 	%f6, %f54, %f58;
	st.local.u32 	[%rd1], %r50;
	cvt.u32.u64 	%r152, %rd9;
	shr.u64 	%rd16, %rd9, 32;
	cvt.u32.u64 	%r151, %rd16;
	cvt.u32.u64 	%r150, %rd10;
	shr.u64 	%rd17, %rd10, 32;
	cvt.u32.u64 	%r149, %rd17;
	cvt.u32.u64 	%r148, %rd11;
	shr.u64 	%rd18, %rd11, 32;
	cvt.u32.u64 	%r147, %rd18;
	mov.f32 	%f9, 0f00000000;
	ld.const.u64 	%rd4, [params+56];
	mov.u32 	%r146, %r50;
	mov.u32 	%r153, %r45;
	mov.u32 	%r154, %r44;
	mov.f32 	%f69, %f9;

$L__BB1_1:
	mov.f32 	%f66, 0f5A0E1BCA;
	mov.u32 	%r86, 255;
	mov.u32 	%r91, 8;
	// begin inline asm
	call(%r154,%r153,%r152,%r151,%r150,%r149,%r148,%r147,%r61,%r62,%r63,%r64,%r65,%r66,%r67,%r68,%r69,%r70,%r71,%r72,%r73,%r74,%r75,%r76,%r77,%r78,%r79,%r80,%r81,%r82,%r83,%r84),_optix_trace_typed_32,(%r50,%rd4,%f1,%f2,%f3,%f4,%f5,%f6,%f69,%f66,%f9,%r86,%r50,%r50,%r50,%r50,%r91,%r154,%r153,%r152,%r151,%r150,%r149,%r148,%r147,%r50,%r50,%r50,%r50,%r50,%r50,%r50,%r50,%r50,%r50,%r50,%r50,%r50,%r50,%r50,%r50,%r50,%r50,%r50,%r50,%r50,%r50,%r50,%r50);
	// end inline asm
	ld.local.f32 	%f68, [%rd3];
	add.ftz.f32 	%f69, %f68, 0f38D1B717;
	ld.local.u32 	%r155, [%rd1];
	setp.ne.s32 	%p1, %r146, %r155;
	setp.lt.s32 	%p2, %r155, 1024;
	and.pred  	%p3, %p2, %p1;
	mov.u32 	%r146, %r155;
	@%p3 bra 	$L__BB1_1;

	setp.lt.s32 	%p4, %r155, 1025;
	@%p4 bra 	$L__BB1_4;

	mov.u32 	%r155, 1024;
	st.local.u32 	[%rd1], %r155;

$L__BB1_4:
	mad.lo.s32 	%r30, %r51, %r45, %r44;
	ld.const.u64 	%rd20, [params+8];
	cvta.to.global.u64 	%rd21, %rd20;
	mul.wide.s32 	%rd22, %r30, 4;
	add.s64 	%rd23, %rd21, %rd22;
	st.global.u32 	[%rd23], %r155;
	setp.lt.s32 	%p5, %r155, 1;
	@%p5 bra 	$L__BB1_11;

	ld.const.u64 	%rd24, [params];
	cvta.to.global.u64 	%rd5, %rd24;
	and.b32  	%r160, %r155, 3;
	add.s32 	%r126, %r155, -1;
	setp.lt.u32 	%p6, %r126, 3;
	mov.u32 	%r158, 0;
	@%p6 bra 	$L__BB1_8;

	sub.s32 	%r157, %r155, %r160;

$L__BB1_7:
	mul.wide.s32 	%rd25, %r158, 4;
	add.s64 	%rd26, %rd2, %rd25;
	ld.local.v4.u32 	{%r128, %r129, %r130, %r131}, [%rd26];
	mul.lo.s32 	%r136, %r51, %r158;
	mad.lo.s32 	%r137, %r136, %r52, %r30;
	mul.wide.u32 	%rd27, %r137, 4;
	add.s64 	%rd28, %rd5, %rd27;
	st.global.u32 	[%rd28], %r128;
	add.s32 	%r138, %r136, %r51;
	mad.lo.s32 	%r139, %r138, %r52, %r30;
	mul.wide.u32 	%rd29, %r139, 4;
	add.s64 	%rd30, %rd5, %rd29;
	st.global.u32 	[%rd30], %r129;
	add.s32 	%r140, %r138, %r51;
	mad.lo.s32 	%r141, %r140, %r52, %r30;
	mul.wide.u32 	%rd31, %r141, 4;
	add.s64 	%rd32, %rd5, %rd31;
	st.global.u32 	[%rd32], %r130;
	add.s32 	%r142, %r140, %r51;
	mad.lo.s32 	%r143, %r142, %r52, %r30;
	mul.wide.u32 	%rd33, %r143, 4;
	add.s64 	%rd34, %rd5, %rd33;
	st.global.u32 	[%rd34], %r131;
	add.s32 	%r158, %r158, 4;
	add.s32 	%r157, %r157, -4;
	setp.ne.s32 	%p7, %r157, 0;
	@%p7 bra 	$L__BB1_7;

$L__BB1_8:
	setp.eq.s32 	%p8, %r160, 0;
	@%p8 bra 	$L__BB1_11;

	mad.lo.s32 	%r144, %r52, %r158, %r45;
	mad.lo.s32 	%r159, %r51, %r144, %r44;
	mul.lo.s32 	%r39, %r51, %r52;
	mul.wide.s32 	%rd35, %r158, 4;
	add.s64 	%rd38, %rd2, %rd35;

$L__BB1_10:
	.pragma "nounroll";
	ld.local.u32 	%r145, [%rd38];
	mul.wide.u32 	%rd36, %r159, 4;
	add.s64 	%rd37, %rd5, %rd36;
	st.global.u32 	[%rd37], %r145;
	add.s32 	%r159, %r159, %r39;
	add.s64 	%rd38, %rd38, 4;
	add.s32 	%r160, %r160, -1;
	setp.ne.s32 	%p9, %r160, 0;
	@%p9 bra 	$L__BB1_10;

$L__BB1_11:
	ret;

}
	// .globl	_ZSt4sqrtf
.visible .func  (.param .b32 func_retval0) _ZSt4sqrtf(
	.param .b32 _ZSt4sqrtf_param_0
)
{
	.reg .f32 	%f<3>;


	ld.param.f32 	%f1, [_ZSt4sqrtf_param_0];
	sqrt.approx.ftz.f32 	%f2, %f1;
	st.param.f32 	[func_retval0+0], %f2;
	ret;

}
	// .globl	__anyhit__ah
.visible .entry __anyhit__ah()
{
	.reg .b32 	%r<14>;


	mov.u32 	%r2, 0;
	// begin inline asm
	call (%r1), _optix_get_payload, (%r2);
	// end inline asm
	mov.u32 	%r4, 1;
	// begin inline asm
	call (%r3), _optix_get_payload, (%r4);
	// end inline asm
	mov.u32 	%r6, 2;
	// begin inline asm
	call (%r5), _optix_get_payload, (%r6);
	// end inline asm
	mov.u32 	%r8, 3;
	// begin inline asm
	call (%r7), _optix_get_payload, (%r8);
	// end inline asm
	mov.u32 	%r10, 4;
	// begin inline asm
	call (%r9), _optix_get_payload, (%r10);
	// end inline asm
	mov.u32 	%r12, 5;
	// begin inline asm
	call (%r11), _optix_get_payload, (%r12);
	// end inline asm
	ret;

}
	// .globl	__closesthit__ch
.visible .entry __closesthit__ch()
{
	.reg .f32 	%f<8>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<12>;


	mov.u32 	%r2, 0;
	// begin inline asm
	call (%r1), _optix_get_payload, (%r2);
	// end inline asm
	mov.u32 	%r4, 1;
	// begin inline asm
	call (%r3), _optix_get_payload, (%r4);
	// end inline asm
	mov.u32 	%r6, 2;
	// begin inline asm
	call (%r5), _optix_get_payload, (%r6);
	// end inline asm
	cvt.u64.u32 	%rd1, %r5;
	mov.u32 	%r8, 3;
	// begin inline asm
	call (%r7), _optix_get_payload, (%r8);
	// end inline asm
	cvt.u64.u32 	%rd2, %r7;
	bfi.b64 	%rd3, %rd2, %rd1, 32, 32;
	mov.u32 	%r10, 4;
	// begin inline asm
	call (%r9), _optix_get_payload, (%r10);
	// end inline asm
	cvt.u64.u32 	%rd4, %r9;
	mov.u32 	%r12, 5;
	// begin inline asm
	call (%r11), _optix_get_payload, (%r12);
	// end inline asm
	cvt.u64.u32 	%rd5, %r11;
	bfi.b64 	%rd6, %rd5, %rd4, 32, 32;
	mov.u32 	%r14, 6;
	// begin inline asm
	call (%r13), _optix_get_payload, (%r14);
	// end inline asm
	cvt.u64.u32 	%rd7, %r13;
	mov.u32 	%r16, 7;
	// begin inline asm
	call (%r15), _optix_get_payload, (%r16);
	// end inline asm
	cvt.u64.u32 	%rd8, %r15;
	bfi.b64 	%rd9, %rd8, %rd7, 32, 32;
	// begin inline asm
	call (%r17), _optix_read_primitive_idx, ();
	// end inline asm
	// begin inline asm
	call (%r19), _optix_get_attribute_1, ();
	// end inline asm
	ld.u32 	%r20, [%rd3];
	add.s32 	%r21, %r20, 1;
	st.u32 	[%rd3], %r21;
	st.u32 	[%rd9], %r19;
	mul.wide.s32 	%rd10, %r20, 4;
	add.s64 	%rd11, %rd6, %rd10;
	st.u32 	[%rd11], %r17;
	ret;

}
	// .globl	__intersection__is
.visible .entry __intersection__is()
{
	.reg .pred 	%p<16>;
	.reg .f32 	%f<39>;
	.reg .b32 	%r<7>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<9>;


	// begin inline asm
	call (%f3), _optix_get_ray_tmin, ();
	// end inline asm
	// begin inline asm
	call (%f4), _optix_get_ray_tmax, ();
	// end inline asm
	// begin inline asm
	call (%rd1), _optix_get_sbt_data_ptr_64, ();
	// end inline asm
	ld.u64 	%rd3, [%rd1];
	// begin inline asm
	call (%rd2), _optix_get_sbt_data_ptr_64, ();
	// end inline asm
	ld.u64 	%rd4, [%rd2+8];
	// begin inline asm
	call (%r1), _optix_read_primitive_idx, ();
	// end inline asm
	// begin inline asm
	call (%f5), _optix_get_world_ray_origin_x, ();
	// end inline asm
	// begin inline asm
	call (%f6), _optix_get_world_ray_origin_y, ();
	// end inline asm
	// begin inline asm
	call (%f7), _optix_get_world_ray_origin_z, ();
	// end inline asm
	// begin inline asm
	call (%f8), _optix_get_world_ray_direction_x, ();
	// end inline asm
	// begin inline asm
	call (%f9), _optix_get_world_ray_direction_y, ();
	// end inline asm
	// begin inline asm
	call (%f10), _optix_get_world_ray_direction_z, ();
	// end inline asm
	mul.wide.u32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	mul.lo.s32 	%r2, %r1, 6;
	mul.wide.u32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.f32 	%f11, [%rd8];
	sub.ftz.f32 	%f12, %f11, %f5;
	div.approx.ftz.f32 	%f13, %f12, %f8;
	ld.f32 	%f14, [%rd8+4];
	sub.ftz.f32 	%f15, %f14, %f6;
	div.approx.ftz.f32 	%f16, %f15, %f9;
	ld.f32 	%f17, [%rd8+8];
	sub.ftz.f32 	%f18, %f17, %f7;
	div.approx.ftz.f32 	%f19, %f18, %f10;
	ld.f32 	%f20, [%rd8+12];
	sub.ftz.f32 	%f21, %f20, %f5;
	div.approx.ftz.f32 	%f22, %f21, %f8;
	ld.f32 	%f23, [%rd8+16];
	sub.ftz.f32 	%f24, %f23, %f6;
	div.approx.ftz.f32 	%f25, %f24, %f9;
	ld.f32 	%f26, [%rd8+20];
	sub.ftz.f32 	%f27, %f26, %f7;
	div.approx.ftz.f32 	%f28, %f27, %f10;
	setp.lt.ftz.f32 	%p1, %f13, %f22;
	selp.f32 	%f29, %f13, %f22, %p1;
	setp.lt.ftz.f32 	%p2, %f16, %f25;
	selp.f32 	%f30, %f16, %f25, %p2;
	setp.lt.ftz.f32 	%p3, %f19, %f28;
	selp.f32 	%f31, %f19, %f28, %p3;
	setp.gt.ftz.f32 	%p4, %f13, %f22;
	selp.f32 	%f32, %f13, %f22, %p4;
	setp.gt.ftz.f32 	%p5, %f16, %f25;
	selp.f32 	%f33, %f16, %f25, %p5;
	setp.gt.ftz.f32 	%p6, %f19, %f28;
	selp.f32 	%f34, %f19, %f28, %p6;
	setp.gt.ftz.f32 	%p7, %f29, %f30;
	selp.f32 	%f35, %f29, %f30, %p7;
	setp.gt.ftz.f32 	%p8, %f35, %f31;
	selp.f32 	%f1, %f35, %f31, %p8;
	setp.lt.ftz.f32 	%p9, %f32, %f33;
	selp.f32 	%f36, %f32, %f33, %p9;
	setp.lt.ftz.f32 	%p10, %f36, %f34;
	selp.f32 	%f2, %f36, %f34, %p10;
	setp.leu.ftz.f32 	%p11, %f2, %f3;
	setp.geu.ftz.f32 	%p12, %f2, %f4;
	or.pred  	%p13, %p11, %p12;
	ld.f32 	%f37, [%rd6];
	cvt.ftz.f64.f32 	%fd1, %f37;
	setp.ltu.f64 	%p14, %fd1, 0d3FC999999999999A;
	or.pred  	%p15, %p14, %p13;
	@%p15 bra 	$L__BB5_2;

	mov.b32 	%r5, %f1;
	mov.b32 	%r6, %f2;
	mov.u32 	%r4, 0;
	// begin inline asm
	call (%r3), _optix_report_intersection_2, (%f2, %r4, %r5, %r6);
	// end inline asm

$L__BB5_2:
	ret;

}
	// .globl	__miss__ms
.visible .entry __miss__ms()
{



	ret;

}
	// .globl	_ZN3cub11EmptyKernelIvEEvv
.visible .entry _ZN3cub11EmptyKernelIvEEvv()
{



	ret;

}
	// .globl	_ZNSt16initializer_listIjEC2EPKjm
.visible .func _ZNSt16initializer_listIjEC2EPKjm(
	.param .b64 _ZNSt16initializer_listIjEC2EPKjm_param_0,
	.param .b64 _ZNSt16initializer_listIjEC2EPKjm_param_1,
	.param .b64 _ZNSt16initializer_listIjEC2EPKjm_param_2
)
{
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_ZNSt16initializer_listIjEC2EPKjm_param_0];
	ld.param.u64 	%rd2, [_ZNSt16initializer_listIjEC2EPKjm_param_1];
	ld.param.u64 	%rd3, [_ZNSt16initializer_listIjEC2EPKjm_param_2];
	st.u64 	[%rd1], %rd2;
	st.u64 	[%rd1+8], %rd3;
	ret;

}

